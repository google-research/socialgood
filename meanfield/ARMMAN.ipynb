{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsBDDrNqZj4F"
      },
      "source": [
        "# Description\n",
        "Compare Whittle index and mean-field approximation using (i) generated transition probabilities and (ii) ARMMAN transition proababilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvsqnEVyfNIF"
      },
      "source": [
        "# Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pulp\n",
        "! conda install pulp"
      ],
      "metadata": {
        "id": "4nyt057f91r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqKauyvLfT1u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.cluster import KMeans\n",
        "import pulp\n",
        "import matplotlib.pyplot as plt\n",
        "import time, datetime\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fonH3t2neaBa"
      },
      "source": [
        "# Generate Data (Choice 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "url3Rx-UZErt"
      },
      "outputs": [],
      "source": [
        "def get_random_RMAB():\n",
        "  # distribution of the beneficiaries over the clusters\n",
        "  Ni = (np.ones(N_CLS)*(N_BEN/N_CLS)).astype(int)\n",
        "  assert(Ni.sum() == N_BEN)\n",
        "\n",
        "  P = np.random.uniform(low=0.1, high=0.9, \n",
        "                        size=(N_CLS, N_STATES, N_ACTIONS, N_STATES))\n",
        "  P /= P.sum(axis=-1, keepdims=1)\n",
        "\n",
        "  R = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  R[:,:,:] = np.random.uniform(size=N_STATES).reshape(1,N_STATES,1)\n",
        "\n",
        "  C = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  cost = np.random.uniform(low=0.1, high=1, size=N_ACTIONS)\n",
        "  cost[0] = 0  # the first action has cost zero\n",
        "  C[:,:,:] = cost.reshape(1,1,N_ACTIONS)\n",
        "\n",
        "  return P, R, C, Ni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpRBc5iOMfJT"
      },
      "source": [
        "# Load ARMMAN Data (Choice 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li5DRTLChTcY"
      },
      "source": [
        "## Convert historical data to transition matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsfO3BLOPYoG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Converts the raw historical behavior of the beneficiaries to transition matrices.\n",
        "If no data is available for a (state,action) pair, the probs have been set to 0.\n",
        "\"\"\"\n",
        "def convert_history_to_TP(rawData):\n",
        "  months = rawData.keys()\n",
        "\n",
        "  features = []; passiveTP = []; activeTP = [];  # keeping active and passive separate because of sparsity in active\n",
        "\n",
        "  for month in months:\n",
        "    for i in range(rawData[month]['features'].shape[0]):\n",
        "      if rawData[month]['isbad'][i]:\n",
        "        continue\n",
        "      features.append(rawData[month]['features'][i])\n",
        "\n",
        "      # transition probs ({NE,E},{NE,E}) for passive and active\n",
        "      states = np.concatenate([rawData[month]['states_pre'][i], rawData[month]['states'][i]])\n",
        "      actions = np.concatenate([np.zeros(rawData[month]['states_pre'][i].shape), rawData[month]['actions'][i]])\n",
        "      assert(len(states) == len(actions)+1)\n",
        "\n",
        "      # What to do when we don't have passive probabilities from a given state?\n",
        "      # If only E, then set (NE,E) to 1? If only NE, then set (E,NE) to 1? CURRENT\n",
        "      # Maybe learn one from the other and the features?\n",
        "      tp_counts = np.zeros((N_ACTIONS, N_STATES, N_STATES))\n",
        "      for a in range(N_ACTIONS):\n",
        "        for s in range(N_STATES):\n",
        "          for sp in range(N_STATES):\n",
        "            for j in range(len(actions)):\n",
        "              if (actions[j], states[j], states[j+1]) == (a, s, sp):\n",
        "                tp_counts[a,s,sp] += 1\n",
        "          # hack for ARMMAN data\n",
        "          if (a == 0) and (tp_counts[a,s].sum() == 0):\n",
        "            tp_counts[0,s,1-s] = 1\n",
        "              \n",
        "      tp_counts /= tp_counts.sum(axis=-1, keepdims=1) + (tp_counts.sum(axis=-1, keepdims=1) == 0)\n",
        "\n",
        "      passiveTP.append(tp_counts[0]); activeTP.append(tp_counts[1])\n",
        "  \n",
        "  features = np.array(features); passiveTP = np.array(passiveTP); activeTP = np.array(activeTP);\n",
        "  assert(features.shape[0] == activeTP.shape[0])\n",
        "\n",
        "  rawTP = {'features': features, 'passiveTP': passiveTP, 'activeTP': activeTP}\n",
        "  return rawTP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b5lyLxPhe40"
      },
      "source": [
        "## Do clustering on passive TP (and estimate active TP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KB7oUnLDVVs"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Clusters the passive transition probabilities.\n",
        "Used in discrete mean-field.\n",
        "\"\"\"\n",
        "def rawTP_to_RMAB(rawTPAll, print_variance=False):\n",
        "  \n",
        "  # if N_BEN is given, slect a random sample of N_BEN benficiaries\n",
        "  global N_BEN\n",
        "  if N_BEN is not None:\n",
        "    selected_bens = np.random.randint(low=0, high=rawTPAll['passiveTP'].shape[0], size=N_BEN)\n",
        "    rawTP = dict()\n",
        "    for key in rawTPAll.keys():\n",
        "      rawTP[key] = rawTPAll[key][selected_bens,:]\n",
        "  else:\n",
        "    N_BEN = rawTPAll['passiveTP'].shape[0]\n",
        "    rawTP = rawTPAll\n",
        "\n",
        "  P = np.zeros((N_CLS, N_ACTIONS, N_STATES, N_STATES))\n",
        "\n",
        "  # get the passive transition probabilities by clustering\n",
        "  pTP = rawTP['passiveTP']\n",
        "  kmeans = KMeans(n_clusters=N_CLS, random_state=0).fit(pTP.reshape((pTP.shape[0], N_STATES**2)))\n",
        "  P[:,0,:,:] = kmeans.cluster_centers_.reshape((N_CLS, N_STATES, N_STATES))\n",
        "\n",
        "  # get the count of beneficiaries in each cluster\n",
        "  Ni = np.zeros(N_CLS)\n",
        "  for i in range(N_CLS):\n",
        "    Ni[i] = (kmeans.labels_ == i).sum()\n",
        "  \n",
        "  # estimate the active probabilitites\n",
        "  activeTP = np.zeros((N_CLS, N_STATES, N_STATES))\n",
        "  activeTPcount = np.zeros(N_CLS)\n",
        "  for i in range(N_CLS):\n",
        "    aTP = rawTP['activeTP'][kmeans.labels_ == i,:]\n",
        "\n",
        "    aTPcounts = (aTP.sum(axis=-1, keepdims=1) > 0.5).sum(axis=0)\n",
        "    aTPcounts += (aTPcounts == 0)  # avoid division by 0\n",
        "    aTP = aTP.sum(axis=0) / aTPcounts\n",
        "    # there may still be no data for some clusters! Set 0.5 0.5 for them?\n",
        "    aTP += 0.5*(aTP.sum(axis=-1, keepdims=1) < 0.5)\n",
        "    P[i,1,:,:] = aTP\n",
        "\n",
        "  R = np.zeros((N_CLS, N_ACTIONS, N_STATES))\n",
        "  R[:,:,:] = np.array([0,1]).reshape(1,1,N_STATES)\n",
        "\n",
        "  # Compute the variance in the assigned and actual transition probabilites\n",
        "  if print_variance == True:\n",
        "    pVars = np.zeros(N_CLS); aVars = np.zeros(N_CLS)\n",
        "    for i in range(N_CLS):\n",
        "      pTP = rawTP['passiveTP'][kmeans.labels_ == i,:]\n",
        "      pVar = (pTP - P[i,0,:,:]).var(axis=0).mean()\n",
        "\n",
        "      aTP = rawTP['activeTP'][kmeans.labels_ == i,:]\n",
        "      aTP0 = aTP[aTP[:,0,:].sum(axis=-1) > 0.5, :]\n",
        "      aTP1 = aTP[aTP[:,1,:].sum(axis=-1) > 0.5, :]\n",
        "      aVar0 = (aTP0 - P[i,1,0,:]).var(axis=0).mean()\n",
        "      if np.isnan(aVar0): aVar0 = 0\n",
        "      aVar1 = (aTP1 - P[i,1,1,:]).var(axis=0).mean()\n",
        "      if np.isnan(aVar1): aVar1 = 0\n",
        "      aVar = (aVar0 + aVar1)/2\n",
        "\n",
        "      print(f'Cluster {i}.\\t #ben = {pTP.shape[0]},\\t #ben with active data = {(aTP0.shape[0], aTP1.shape[0])}\\t', end=\" \")\n",
        "      print(f'Variance. Passive = {pVar},\\t Active = {aVar} ')\n",
        "\n",
        "      pVars[i] = pVar; aVars[i] = aVar\n",
        "\n",
        "    print(f'Average Variance. Passive = {pVars.mean()},\\t Active = {aVars.mean()}')\n",
        "    print(f'Maximum Variance. Passive = {pVars.max()},\\t Active = {aVars.max()}')\n",
        "\n",
        "  return P, R, Ni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRKlu7GA4zTX"
      },
      "source": [
        "## Load from local drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWyYrc6JzYKZ"
      },
      "outputs": [],
      "source": [
        "def get_ARMMAN_RMAB():\n",
        "  with open(f'{FILE_PREFIX}/data/jan-march-may-data-processed.pickle', 'rb') as handle:\n",
        "    rawData = pickle.load(handle)\n",
        "\n",
        "  try:\n",
        "    with open(f'{FILE_PREFIX}/data/jan-march-may-rawTP.pickle', 'rb') as handle:\n",
        "      rawTP = pickle.load(handle)\n",
        "  except:\n",
        "    rawTP = convert_history_to_TP(rawData)\n",
        "    with open(f'{FILE_PREFIX}/data/jan-march-may-rawTP.pickle', 'wb') as handle:\n",
        "      pickle.dump(rawTP, handle)\n",
        "  \n",
        "  Pold, Rold, Ni = rawTP_to_RMAB(rawTP)\n",
        "\n",
        "  # change notation from (cls, act, state, ...) to (cls, state, act, ...) to\n",
        "  # match Jackson's AAMAS code notation.\n",
        "  P = np.zeros((N_CLS, N_STATES, N_ACTIONS, N_STATES))\n",
        "  R = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  for i in range(N_CLS):\n",
        "    for s in range(N_STATES):\n",
        "      for a in range(N_ACTIONS):\n",
        "        P[i,s,a,:] = Pold[i,a,s,:]\n",
        "        R[i,s,a] = Rold[i,a,s]\n",
        "\n",
        "  # Construct the cost matrix used by mult-action algorithms\n",
        "  C = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  assert(N_ACTIONS == 2)\n",
        "  C[:,:,1] = 1\n",
        "  \n",
        "  return P, R, C, Ni"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Adverse Data for Whittle (Choice 3)"
      ],
      "metadata": {
        "id": "P1Q1tOEeAoab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_adverse_RMAB_1():\n",
        "  # does not work for only 1 benificiary\n",
        "  Ni = np.array([N_BEN])\n",
        "\n",
        "  P = np.zeros((N_CLS, N_STATES, N_ACTIONS, N_STATES))\n",
        "  # START-1\n",
        "  P[:,0,0] = np.array([0,0.1,0,0,0.9])\n",
        "  P[:,0,1] = np.array([0,1,0,0,0])\n",
        "\n",
        "  # ENGAGED-1\n",
        "  P[:,1,0] = np.array([0,0,0,0,1])\n",
        "  P[:,1,1] = np.array([0, 0.9, 0, 0, 0.1])\n",
        "\n",
        "  # START-2\n",
        "  P[:,2,0] = np.array([0,0,0,0.1,0.9])\n",
        "  P[:,2,1] = np.array([0,0,0,1,0])\n",
        "\n",
        "  # ENGAGED-2\n",
        "  P[:,3,0] = np.array([0,0,0,0,1])\n",
        "  P[:,3,1] = np.array([0,0,0,0,1])\n",
        "\n",
        "  # DROPOUT\n",
        "  P[:,4,0] = np.array([0.1, 0, 0.1, 0, 0.8])\n",
        "  P[:,4,1] = np.array([0.1, 0, 0.1, 0, 0.8])\n",
        "\n",
        "  R = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  R[0,:,:] = np.array([0, 0.95, 0, 1, 0]).reshape((1,N_STATES,1))\n",
        "\n",
        "  C = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  C[:,:,:] = np.array([0,1]).reshape(1,1,N_ACTIONS)\n",
        "\n",
        "  return P, R, C, Ni"
      ],
      "metadata": {
        "id": "vtkUpFZ6EeWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_adverse_RMAB_2():\n",
        "  assert False, \"Obsolete\"\n",
        "  eps = 1e-3\n",
        "  # works for 2 beneficiaries\n",
        "  Ni = np.array([1,1])\n",
        "\n",
        "  P = np.zeros((N_CLS, N_STATES, N_ACTIONS, N_STATES))\n",
        "  P[:,:,:] = np.array([0,0,1])\n",
        "  P[0,0,1] = np.array([0,1,0])\n",
        "  P[1,0,1] = np.array([0,1,0])\n",
        "  P[1,1,1] = np.array([0,1,0])\n",
        "\n",
        "  R = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  R[0,:,:] = np.array([0,1,0]).reshape((1,N_STATES,1))\n",
        "  R[1,:,:] = np.array([0,1-eps,0]).reshape((1,N_STATES,1))\n",
        "\n",
        "  C = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  C[:,:,:] = np.array([0,1]).reshape(1,1,N_ACTIONS)\n",
        "\n",
        "  return P, R, C, Ni"
      ],
      "metadata": {
        "id": "__PJGPYJAr_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_adverse_RMAB():\n",
        "  return get_adverse_RMAB_1()"
      ],
      "metadata": {
        "id": "ISMYtW4jFyDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guQ6iQFP29n6"
      },
      "source": [
        "# Whittle index\n",
        "Computed for finite horizon H. Currently doing backtracking, a faster algorithm may exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54y-2dNB5YRJ"
      },
      "outputs": [],
      "source": [
        "TOL = 1e-3\n",
        "\n",
        "def q_value_finite(P, R, H):\n",
        "  \"\"\"\n",
        "  Finds the q-value by backtracking.\n",
        "  \"\"\"\n",
        "  q0 = R;  # notice we are in notation (s,a,s') as in the lit\n",
        "  t = H\n",
        "  while (t > 0):\n",
        "    q1 = q0.copy()\n",
        "    v = q0.max(axis=-1)\n",
        "    q0 = R + P.dot(v)\n",
        "    t -= 1\n",
        "  \n",
        "  return q0\n",
        "\n",
        "\n",
        "def q_value_infinite(P, R, gamma):\n",
        "  \"\"\"\n",
        "  Finds the q-value by iterative method.\n",
        "  \"\"\"\n",
        "  q0 = R;  # notice we are in notation (s,a,s') as in the lit\n",
        "  q1 = np.zeros(R.shape)\n",
        "  while (np.linalg.norm(q1-q0) > TOL*q0.size):\n",
        "    q1 = q0.copy()\n",
        "    v = q0.max(axis=-1)\n",
        "    q0 = R + gamma*P.dot(v)\n",
        "  \n",
        "  return q0\n",
        "\n",
        "\n",
        "def whittle_MDP(P, R, H, s, gamma, horizon_type):\n",
        "  \"\"\"\n",
        "  Given an MDP with transition matrices P (s,a,s) and rewards R (s,a)\n",
        "  computes the Whittle index for each state. \n",
        "  H is the horizon. \n",
        "  s the state for which we are computing WI.\n",
        "  Uses binary search for the index and backtracking for finding Q-values.\n",
        "  \"\"\"\n",
        "  assert(P.shape == (N_STATES, N_ACTIONS, N_STATES))\n",
        "  assert(R.shape == (N_STATES, N_ACTIONS))\n",
        "\n",
        "  # do binary search over ld (lambda)\n",
        "  ld_min = -1e5; ld_max = 1e5; ld = 0\n",
        "\n",
        "  qDiff = 1e5\n",
        "  while np.abs(qDiff) >= TOL:\n",
        "    ld = (ld_min + ld_max)/2\n",
        "    Rld = R.copy(); Rld[:,1] = Rld[:,1] - ld\n",
        "    \n",
        "    if horizon_type == \"finite\":\n",
        "      q = q_value_finite(P, Rld, H)\n",
        "    else:\n",
        "      assert horizon_type == \"infinite\"\n",
        "      q = q_value_infinite(P, Rld, gamma)\n",
        "\n",
        "    qDiff = q[s,1] - q[s,0];\n",
        "    if qDiff > TOL:\n",
        "      ld_min = ld\n",
        "    elif qDiff < TOL:\n",
        "      ld_max = ld\n",
        "\n",
        "  return ld\n",
        "\n",
        "\n",
        "def whittle_RMAB(P, R, H, gamma, horizon_type):\n",
        "  assert ((N_CLS, N_STATES, N_ACTIONS, N_STATES) == P.shape)\n",
        "\n",
        "  WI = np.zeros((N_CLS, N_STATES))\n",
        "  \n",
        "  for i in range(N_CLS):\n",
        "    for s in range(N_STATES):\n",
        "      WI[i,s] = whittle_MDP(P[i,:], R[i,:], H, s, gamma, horizon_type)\n",
        "  \n",
        "  return WI\n",
        "\n",
        "\n",
        "def whittle_action(WI, state, Ni):\n",
        "  state_shape = state.shape\n",
        "  state = state.reshape(state.size)\n",
        "  action = np.zeros(state.size, dtype=int)\n",
        "  wi = WI.reshape(state.size).copy()\n",
        "\n",
        "  remaining = BUDGET\n",
        "  while remaining > 0:\n",
        "    i_ = np.argmax(wi)\n",
        "    action[i_] += min(remaining, np.floor(state[i_]))\n",
        "    remaining -= action[i_]\n",
        "    wi[i_] = -np.inf\n",
        "  \n",
        "  return action.reshape(state_shape)\n",
        "\n",
        "\n",
        "def whittle(P, R, C, H, state, Ni, gamma, horizon_type):\n",
        "  assert (N_ACTIONS == 2)  # Whittle works only for two actions\n",
        "  WI = whittle_RMAB(P, R, H, gamma, horizon_type)\n",
        "  action = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  action[:,:,1] = whittle_action(WI, state, Ni)\n",
        "  action[:,:,0] = state - action[:,:,1]\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def whittle_finite(*args, **kwargs):\n",
        "  return whittle(*args, **kwargs, horizon_type=\"finite\")\n",
        "\n",
        "def whittle_infinite(*args, **kwargs):\n",
        "  return whittle(*args, **kwargs, horizon_type=\"infinite\")"
      ],
      "metadata": {
        "id": "5G3Hf-m2ndes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_sUVCbe3BGd"
      },
      "source": [
        "# Mean-Field"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LP and LP rounding\n",
        "Maintain only one copy throughout all experiments, which is this."
      ],
      "metadata": {
        "id": "P_q5iemCbPKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LP"
      ],
      "metadata": {
        "id": "snw89a9Zdp5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PuLP - COIN"
      ],
      "metadata": {
        "id": "KltJjJdNmRfR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHUmiaCphKnj"
      },
      "outputs": [],
      "source": [
        "def mean_field_distribution(P, R, C, B, H, state, gamma,\n",
        "                            sleeping_constraint, available_arms, sleeping_weeks):\n",
        "  assert( (H-1, N_CLS, N_STATES, N_ACTIONS, N_STATES) == P.shape )\n",
        "  assert( (N_CLS, N_STATES, N_ACTIONS) == R.shape )\n",
        "  assert( (N_CLS, N_STATES, N_ACTIONS) == C.shape )\n",
        "  assert( (H,) == B.shape )\n",
        "  assert( (N_CLS, N_STATES) == state.shape )\n",
        "\n",
        "  # the LP problem\n",
        "  LP = pulp.LpProblem(\"Mean_Field\", pulp.LpMaximize)\n",
        "\n",
        "  mu = pulp.LpVariable.dicts(\"mu\", (range(H),range(N_CLS),range(N_STATES)),\n",
        "                            0, N_BEN, pulp.LpContinuous)\n",
        "  alpha = pulp.LpVariable.dicts(\"alpha\", (range(H),range(N_CLS),range(N_STATES),range(N_ACTIONS)),\n",
        "                            0, N_BEN, pulp.LpContinuous)\n",
        "\n",
        "  # Objective\n",
        "  LP += (\n",
        "      pulp.lpSum(alpha[t][i][s][a]*R[i,s,a]* (gamma**t)\n",
        "                for t in range(H)\n",
        "                for i in range(N_CLS)\n",
        "                for s in range(N_STATES)\n",
        "                for a in range(N_ACTIONS))\n",
        "  )\n",
        "\n",
        "  # Constraints\n",
        "  # feasibility (equality)\n",
        "  for t in range(H):\n",
        "    for i in range(N_CLS):\n",
        "      for s in range(N_STATES):\n",
        "        LP += (\n",
        "            pulp.lpSum(alpha[t][i][s][a] for a in range(N_ACTIONS)) == mu[t][i][s],\n",
        "            f\"feasibility for time {t}, arm {i}, state {s}\"\n",
        "        )\n",
        "\n",
        "  # dynamics (equality)\n",
        "  for t in range(H-1):\n",
        "    for i in range(N_CLS):\n",
        "      for sp in range(N_STATES):\n",
        "        LP += (\n",
        "            mu[t+1][i][sp] == pulp.lpSum(\n",
        "                alpha[t][i][s][a]*P[t,i,s,a,sp]\n",
        "                for s in range(N_STATES)\n",
        "                for a in range(N_ACTIONS)\n",
        "            ),\n",
        "            f\"transition dynamics for time {t}, arm {i}, state {sp}\"\n",
        "        )\n",
        "\n",
        "  # budget (inequality)\n",
        "  for t in range(H):\n",
        "    LP += (\n",
        "        pulp.lpSum(alpha[t][i][s][a]*C[i][s][a]\n",
        "                  for i in range(N_CLS)\n",
        "                  for s in range(N_STATES)\n",
        "                  for a in range(N_ACTIONS)) <= B[t],\n",
        "        f\"budget constraint for time {t}\"\n",
        "    )\n",
        "\n",
        "  # sleeping constraint\n",
        "  if sleeping_constraint == True:\n",
        "    # implemented only for one beneficiary per cluster & two actions\n",
        "    assert N_CLS == N_BEN\n",
        "    assert N_ACTIONS == 2\n",
        "    assert sleeping_weeks > 0\n",
        "    for i in range(N_CLS):\n",
        "      for t in range(H):\n",
        "        LP += (\n",
        "            pulp.lpSum(alpha[t+sleep][i][s][1]\n",
        "                        for sleep in range(np.minimum(sleeping_weeks+1,H-t))\n",
        "                        for s in range(N_STATES)) <= 1,\n",
        "            f\"sleeping constraint for {i} in time duration [{t}...{t+sleeping_weeks}]\"\n",
        "        )\n",
        "\n",
        "  # sleeping constraint 2\n",
        "  if available_arms is not None:\n",
        "    # implemented only for one beneficiary per cluster & two actions\n",
        "    assert N_CLS == N_BEN\n",
        "    assert N_ACTIONS == 2\n",
        "    for i in range(N_CLS):\n",
        "      for sleep in range(np.minimum(1-int(available_arms[i]), H)):\n",
        "        for s in range(N_STATES):\n",
        "          LP += (\n",
        "              alpha[sleep][i][s][1] == 0\n",
        "          )\n",
        "\n",
        "  # initialization (equality)\n",
        "  for i in range(N_CLS):\n",
        "    for s in range(N_STATES):\n",
        "      LP += (\n",
        "          mu[0][i][s] == state[i][s],\n",
        "          f\"initialization for time 0, arm {i}, state {s}\"\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "  LP.solve(pulp.PULP_CBC_CMD(msg=False))\n",
        "\n",
        "  # compute the numpy action\n",
        "  alphaNow = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  for i in range(N_CLS):\n",
        "    for s in range(N_STATES):\n",
        "      for a in range(N_ACTIONS):\n",
        "        alphaNow[i][s][a] = alpha[0][i][s][a].varValue\n",
        "\n",
        "  # due to some reason, likely floating point approximation in the LP solver,\n",
        "  # action may have small negative values, which causes runtime errors later.\n",
        "  # Set them to 0.\n",
        "  alphaNow = np.maximum(alphaNow, 0)\n",
        "\n",
        "  return alphaNow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gurobi"
      ],
      "metadata": {
        "id": "BwG5TMYzmaJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_field_distribution_gurobi(P, R, C, B, H, state, gamma,\n",
        "                            sleeping_constraint, available_arms, sleeping_weeks):\n",
        "  assert( (H-1, N_CLS, N_STATES, N_ACTIONS, N_STATES) == P.shape )\n",
        "  assert( (N_CLS, N_STATES, N_ACTIONS) == R.shape )\n",
        "  assert( (N_CLS, N_STATES, N_ACTIONS) == C.shape )\n",
        "  assert( (H,) == B.shape )\n",
        "  assert( (N_CLS, N_STATES) == state.shape )\n",
        "\n",
        "  # the LP problem\n",
        "  LP = gb.Model(\"Mean_Field\")\n",
        "  LP.modelSense = gb.GRB.MAXIMIZE\n",
        "\n",
        "  mu = np.zeros((H, N_CLS, N_STATES), dtype=object)\n",
        "  alpha = np.zeros((H, N_CLS, N_STATES, N_ACTIONS), dtype=object)\n",
        "\n",
        "  for t in range(H):\n",
        "    for i in range(N_CLS):\n",
        "      for s in range(N_STATES):\n",
        "        mu[t,i,s] = LP.addVar(vtype=gb.GRB.CONTINUOUS, lb=0, ub=N_BEN, name=f\"mu_{t}_{i}_{s}\")\n",
        "        for a in range(N_ACTIONS):\n",
        "          alpha[t,i,s,a] = LP.addVar(vtype=gb.GRB.CONTINUOUS, lb=0, ub=N_BEN,\n",
        "                                     name=f\"alpha_{t}_{i}_{s}_{a}\")\n",
        "\n",
        "  # Objective\n",
        "  LP.setObjective(\n",
        "      gb.quicksum(alpha[t][i][s][a]*R[i,s,a]* (gamma**t)\n",
        "                for t in range(H)\n",
        "                for i in range(N_CLS)\n",
        "                for s in range(N_STATES)\n",
        "                for a in range(N_ACTIONS))\n",
        "    )\n",
        "\n",
        "  # Constraints\n",
        "  # feasibility (equality)\n",
        "  for t in range(H):\n",
        "    for i in range(N_CLS):\n",
        "      for s in range(N_STATES):\n",
        "        LP.addConstr(\n",
        "            gb.quicksum(alpha[t][i][s][a] for a in range(N_ACTIONS)) == mu[t][i][s],\n",
        "            f\"feasibility for time {t}, arm {i}, state {s}\"\n",
        "        )\n",
        "\n",
        "  # dynamics (equality)\n",
        "  for t in range(H-1):\n",
        "    for i in range(N_CLS):\n",
        "      for sp in range(N_STATES):\n",
        "        LP.addConstr(\n",
        "            mu[t+1][i][sp] == gb.quicksum(\n",
        "                alpha[t][i][s][a]*P[t,i,s,a,sp]\n",
        "                for s in range(N_STATES)\n",
        "                for a in range(N_ACTIONS)\n",
        "            ),\n",
        "            f\"transition dynamics for time {t}, arm {i}, state {sp}\"\n",
        "        )\n",
        "\n",
        "  # budget (inequality)\n",
        "  for t in range(H):\n",
        "    LP.addConstr(\n",
        "        gb.quicksum(alpha[t][i][s][a]*C[i][s][a]\n",
        "                  for i in range(N_CLS)\n",
        "                  for s in range(N_STATES)\n",
        "                  for a in range(N_ACTIONS)) <= B[t],\n",
        "        f\"budget constraint for time {t}\"\n",
        "    )\n",
        "\n",
        "  # sleeping constraint\n",
        "  if sleeping_constraint == True:\n",
        "    # implemented only for one beneficiary per cluster & two actions\n",
        "    assert N_CLS == N_BEN\n",
        "    assert N_ACTIONS == 2\n",
        "    assert sleeping_weeks > 0\n",
        "    for i in range(N_CLS):\n",
        "      for t in range(H):\n",
        "        LP.addConstr(\n",
        "            gb.quicksum(alpha[t+sleep][i][s][1]\n",
        "                        for sleep in range(np.minimum(sleeping_weeks+1,H-t))\n",
        "                        for s in range(N_STATES)) <= 1,\n",
        "            f\"sleeping constraint for {i} in time duration [{t}...{t+sleeping_weeks}]\"\n",
        "        )\n",
        "\n",
        "  # sleeping constraint (borrowed from past time-steps)\n",
        "  if available_arms is not None:\n",
        "    # implemented only for one beneficiary per cluster & two actions\n",
        "    assert N_CLS == N_BEN\n",
        "    assert N_ACTIONS == 2\n",
        "    for i in range(N_CLS):\n",
        "      for sleep in range(np.minimum(1-int(available_arms[i]), H)):\n",
        "        for s in range(N_STATES):\n",
        "          LP.addConstr(\n",
        "              alpha[sleep][i][s][1] == 0\n",
        "          )\n",
        "\n",
        "  # initialization (equality)\n",
        "  for i in range(N_CLS):\n",
        "    for s in range(N_STATES):\n",
        "      LP.addConstr(\n",
        "          mu[0][i][s] == state[i][s],\n",
        "          f\"initialization for time 0, arm {i}, state {s}\"\n",
        "      )\n",
        "\n",
        "\n",
        "  LP.setParam('OutputFlag', False)\n",
        "  LP.optimize()\n",
        "\n",
        "  # compute the numpy action\n",
        "  alphaNow = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  for v in LP.getVars():\n",
        "    if 'alpha' in v.varName:\n",
        "      t = int(v.varName.split('_')[1])\n",
        "      i = int(v.varName.split('_')[2])\n",
        "      s = int(v.varName.split('_')[3])\n",
        "      a = int(v.varName.split('_')[4])\n",
        "      if t == 0:\n",
        "        alphaNow[i][s][a] = v.x\n",
        "\n",
        "  # due to some reason, likely floating point approximation in the LP solver,\n",
        "  # action may have small negative values, which causes runtime errors later.\n",
        "  # Set them to 0.\n",
        "  alphaNow = np.maximum(alphaNow, 0)\n",
        "\n",
        "  return alphaNow"
      ],
      "metadata": {
        "id": "2GgAgwZNmgVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LP rounding to get action"
      ],
      "metadata": {
        "id": "2KFnMhbtdxHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_field_action(alphaNow, state, C, sleeping_constraint, budget):\n",
        "  # compute the discrete action (intervention) out of alpha\n",
        "  # there are some quick hacks below due to fractional solution & rounding\n",
        "\n",
        "  action = alphaNow.round().astype(int)\n",
        "\n",
        "  # too many actions for a state\n",
        "  for i in range(N_CLS):\n",
        "    for s in range(N_STATES):\n",
        "      action[i,s,0] += state[i,s] - action[i,s].sum()\n",
        "      if action[i,s,0] < 0:\n",
        "        # randomly select an action and decrease the count by 1\n",
        "        # print(\"randomly select an action and decrease the count by 1\")\n",
        "        a = 1 + np.random.choice(np.arange(N_ACTIONS-1), p=action[i,s,1:]/action[i,s,1:].sum())\n",
        "        action[i,s,a] -= 1\n",
        "        action[i,s,0] += 1\n",
        "\n",
        "  assert((action.sum(axis=-1) == state).all() and \"1\")\n",
        "\n",
        "  # too many actions for budget\n",
        "  too_many_actions = 0\n",
        "  while (action*C).sum() > budget + 1e-3:\n",
        "    p = action.copy()\n",
        "    p[:,:,0] = 0  # setting 0 probability for cost 0 action (first one)\n",
        "    p = p.reshape(p.size)\n",
        "    ben = np.random.choice(np.arange(p.size), p = p/p.sum())\n",
        "    i, s, a = np.unravel_index(ben, (N_CLS, N_STATES, N_ACTIONS))\n",
        "    assert(a > 0)\n",
        "    if action[i,s,a] > 0:\n",
        "      action[i,s,a] -= 1\n",
        "      action[i,s,0] += 1\n",
        "    too_many_actions += 1\n",
        "    # print(f\"too_many_actions = {too_many_actions}\")\n",
        "\n",
        "  assert((action.sum(axis=-1) == state).all() and \"2\")\n",
        "\n",
        "  # too few actions (only do if no sleeping constraint)\n",
        "  if sleeping_constraint == False:\n",
        "    # TODO: this can mess up with the sleeping constraint, avoid for now\n",
        "    too_few_action = 0\n",
        "    while (action*C).sum() < budget - 1e-3:\n",
        "      p = action[:,:,0].copy()\n",
        "      if p.sum() < 1:\n",
        "        break  # too much budget\n",
        "      p = p.reshape(p.size)\n",
        "      ben = np.random.choice(np.arange(p.size), p=p/p.sum())\n",
        "      i, s = np.unravel_index(ben, (N_CLS, N_STATES))\n",
        "      a = np.random.randint(low=1, high=N_ACTIONS)\n",
        "      action[i,s,a] += 1\n",
        "      action[i,s,0] -= 1\n",
        "      if (action*C).sum() > budget:\n",
        "        action[i,s,a] -= 1\n",
        "        action[i,s,0] += 1\n",
        "        break\n",
        "      too_few_action += 1\n",
        "      # print(f\"too_few_action = {too_few_action}\")\n",
        "\n",
        "  assert((action.sum(axis=-1) == state).all() and \"3\")\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "uXrIuDx5d3Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MF wrappers for different experiments"
      ],
      "metadata": {
        "id": "_rsiHCptbnJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MF helper functions"
      ],
      "metadata": {
        "id": "SJ4sogGUeT2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_field_action_to_raw_action(cluster_labels, current_state_raw, action):\n",
        "  if cluster_labels is None:\n",
        "    # did not do any clustering, only one player per cluster\n",
        "    # action for the player, action for everyone in the cluster\n",
        "    action_raw = action.sum(axis=1)\n",
        "    # one-hot to action index\n",
        "    action_raw = action_raw.argmax(axis=-1)\n",
        "  else:\n",
        "    action_raw = np.zeros(N_BEN, dtype=int)\n",
        "    for i in range(N_CLS):\n",
        "      for s in range(N_STATES):\n",
        "        # beneficiaries in this cluster and state\n",
        "        bens = (np.arange(N_BEN))[(cluster_labels == i) & (current_state_raw == s)]\n",
        "        # actions for this cluster and state\n",
        "        acts = []; [ acts.extend([a]*action[i,s,a]) for a in range(N_ACTIONS) ]\n",
        "        # randomly distribute the actions among the beneficiaries\n",
        "        acts = np.random.permutation(acts)\n",
        "        assert(acts.shape == bens.shape)\n",
        "        action_raw[bens] = acts\n",
        "\n",
        "  return action_raw"
      ],
      "metadata": {
        "id": "T6WpbOcPeZDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_field_data_stat_to_non_stat(Pstat, H):\n",
        "  P = np.zeros((H-1,) + Pstat.shape)\n",
        "  P[:,:,:,:,:] = Pstat.reshape((1,) + Pstat.shape)\n",
        "\n",
        "  B = np.zeros(H)\n",
        "  B[:-1] = BUDGET\n",
        "\n",
        "  return P, B"
      ],
      "metadata": {
        "id": "pcDAXybhTzrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MF wrapper for my implementation (used vs Whittle)\n",
        "\n"
      ],
      "metadata": {
        "id": "cR9RD5crbygu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_field(P, R, C, H, state, Ni, gamma):\n",
        "  P, B = mean_field_data_stat_to_non_stat(P, H)\n",
        "  \n",
        "  alphaNow = mean_field_distribution(P=P, R=R, C=C, B=B, H=H, state=state, \n",
        "                                     gamma=gamma, sleeping_constraint=False, \n",
        "                                     available_arms=None, sleeping_weeks=None)\n",
        "  action = mean_field_action(alphaNow=alphaNow, state=state, C=C, \n",
        "                             sleeping_constraint=False, budget=B[0])\n",
        "  return action"
      ],
      "metadata": {
        "id": "2sxCYDawcEbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MF wrapper for non-stationary"
      ],
      "metadata": {
        "id": "J1mTja1Lh9Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_field_ns_wrapper(states, P_stat, klist, timestep=0, P_ns=None,\n",
        "                          sleeping_constraint=True, available_arms=None, \n",
        "                          sleeping_weeks=3, clusters=None, gamma=0.99):\n",
        "  \n",
        "  # Modify Global constant (as they are not defined). \n",
        "  # This function is used only in Aditya's code.\n",
        "  for var in ['N_BEN', 'N_CLS', 'N_ACTIONS', 'N_STATES', 'BUDGET']:\n",
        "    assert var not in globals(), f\"{var} should not have been defined.\"\n",
        "  global N_BEN, N_CLS, N_ACTIONS, N_STATES  #, BUDGET no more required\n",
        "    \n",
        "  print(timestep, end=\" \"); sys.stdout.flush()\n",
        "  \n",
        "  (N_BEN, N_ACTIONS, N_STATES, _) = P_stat.shape\n",
        "\n",
        "  if clusters is None:\n",
        "    N_CLS = N_BEN\n",
        "    clusters = np.arange(N_CLS).astype(int)\n",
        "  else:\n",
        "    assert clusters.shape == (N_BEN,)\n",
        "    assert sleeping_constraint == False\n",
        "    assert available_arms is None\n",
        "    clusters = clusters.astype(int)\n",
        "    N_CLS = int(clusters.max()) + 1\n",
        "\n",
        "  H = klist.shape[0] + 1 - timestep\n",
        "\n",
        "  assert (N_BEN,) == states.shape\n",
        "  assert (N_BEN, N_ACTIONS, N_STATES, N_STATES) == P_stat.shape\n",
        "  assert timestep <= klist.shape[0]\n",
        "\n",
        "  P = np.zeros((H-1, N_CLS, N_ACTIONS, N_STATES, N_STATES))\n",
        "  R = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "  C = np.zeros((N_CLS, N_STATES, N_ACTIONS))\n",
        "\n",
        "  for i in range(N_CLS):\n",
        "    if P_ns is None:\n",
        "      P_temp = P_stat.reshape((1, N_BEN, N_ACTIONS, N_STATES, N_STATES))\n",
        "    else:\n",
        "      assert (N_BEN, klist.shape[0], N_ACTIONS, N_STATES, N_STATES) == P_ns.shape\n",
        "      P_temp = P_ns.swapaxes(0,1).copy()\n",
        "      P_temp = P_temp[timestep:,:,:,:,:]\n",
        "    P[:,i,:,:,:] = P_temp[:,clusters==i,:,:,:].mean(axis=1)\n",
        "    R[:,1,:] = 1\n",
        "    C[:,:,1] = 1\n",
        "\n",
        "  P = P.swapaxes(2,3)\n",
        "  assert (H-1, N_CLS, N_STATES, N_ACTIONS, N_STATES) == P.shape\n",
        "\n",
        "  B = np.zeros(H)\n",
        "  B[:-1] = klist[timestep:]\n",
        "\n",
        "  state = np.zeros((N_CLS, N_STATES)).astype(int)\n",
        "  states = states.astype(int)\n",
        "  for i_ben in range(N_BEN):\n",
        "    state[clusters[i_ben], states[i_ben]] += 1\n",
        "\n",
        "  if N_STATES == 2 and N_CLS == N_BEN: assert (state[:,1] == states).all()\n",
        "  \n",
        "  alphaNow = mean_field_distribution(P=P, R=R, C=C, B=B, H=H, state=state, \n",
        "                                     gamma=gamma, \n",
        "                                     sleeping_constraint=sleeping_constraint, \n",
        "                                     available_arms=available_arms, \n",
        "                                     sleeping_weeks=sleeping_weeks)\n",
        "  \n",
        "  action = mean_field_action(alphaNow=alphaNow, state=state, C=C, \n",
        "                             sleeping_constraint=sleeping_constraint, \n",
        "                             budget=B[0])\n",
        "\n",
        "  actions = mean_field_action_to_raw_action(cluster_labels=clusters,\n",
        "                                            current_state_raw=states, \n",
        "                                            action=action)\n",
        "\n",
        "  assert actions.sum() <= klist[timestep] + 0.1, f\"Not respecting the budget {actions.sum()} vs {klist[timestep]}.\"\n",
        "  if available_arms is not None: \n",
        "    assert (actions < np.maximum(0,available_arms) + 0.1).all(), \"Not respecting availability.\"\n",
        "\n",
        "  return actions"
      ],
      "metadata": {
        "id": "uIZ7zDFlGlHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wUdHA8dub8wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random\n",
        "Selects the beneficiaries randomly"
      ],
      "metadata": {
        "id": "fXOWZX5Haqtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random(P, R, C, H, state, Ni, gamma):\n",
        "  action = np.zeros((N_CLS, N_STATES, N_ACTIONS), dtype=int)\n",
        "  action[:,:,0] = state  # put every action as 0\n",
        "\n",
        "  while (action*C).sum() < BUDGET - 1e-3:\n",
        "    p = action[:,:,0].reshape(N_CLS*N_STATES)\n",
        "    if p.sum() < 1:\n",
        "      break  # too much budget\n",
        "    ben = np.random.choice(np.arange(p.size), p=p/p.sum())\n",
        "    i, s = np.unravel_index(ben, (N_CLS, N_STATES))\n",
        "    a = np.random.randint(low=1, high=N_ACTIONS)\n",
        "    action[i,s,a] += 1\n",
        "    action[i,s,0] -= 1\n",
        "    if (action*C).sum() > BUDGET:\n",
        "      action[i,s,a] -= 1\n",
        "      action[i,s,0] += 1\n",
        "      break\n",
        "  \n",
        "  assert((action.sum(axis=-1) == state).all())\n",
        "\n",
        "  return action\n"
      ],
      "metadata": {
        "id": "CTZfemqpazAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzoaFDkMua8M"
      },
      "source": [
        "# Run the Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWdtWmPuroQW"
      },
      "source": [
        "## helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO74YGav3tPo"
      },
      "outputs": [],
      "source": [
        "def get_random_start_state(Ni):\n",
        "  state = np.zeros((N_CLS, N_STATES))\n",
        "  for i in range(N_CLS):\n",
        "    state[i,:] = np.random.multinomial(Ni[i], pvals=np.ones(N_STATES)/N_STATES)\n",
        "\n",
        "  return state.astype(int)\n",
        "\n",
        "def compute_reward(alg, R, state, action):\n",
        "  # the action array represents the num of active actions for each state\n",
        "  assert(R.shape == action.shape)\n",
        "  reward = (R*action).sum()\n",
        "  return reward\n",
        "\n",
        "def next_state(P, state, action):\n",
        "  assert ((N_CLS, N_STATES, N_ACTIONS, N_STATES) == P.shape)\n",
        "  assert((action.sum(axis=-1) == state).all())\n",
        "\n",
        "  next_state = np.zeros(state.shape).astype(int)\n",
        "  for i in range(N_CLS):\n",
        "    for s in range(N_STATES):\n",
        "      for a in range(N_ACTIONS):\n",
        "        next_state[i] += np.random.multinomial(action[i,s,a], pvals=P[i,s,a,:]/P[i,s,a,:].sum())\n",
        "  \n",
        "  assert((state.sum(axis=-1) == next_state.sum(axis=-1)).all())\n",
        "  return next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save backup"
      ],
      "metadata": {
        "id": "OoRzZUXkgEdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_backup():\n",
        "  with open(f'{FILE_PREFIX}/data/backup/rewards_{DATA}_runs_{N_RUNS}_bens_{N_BEN}_budget_{BUDGET}_clusters_{N_CLS}_states_{N_STATES}_actions_{N_ACTIONS}_horizon_{H}_gamma_{GAMMA}.pickle', 'wb') as handle:\n",
        "    pickle.dump((H_RANGE, rewards), handle)\n",
        "\n",
        "  with open(f'{FILE_PREFIX}/data/backup/time_taken_{DATA}_runs_{N_RUNS}_bens_{N_BEN}_budget_{BUDGET}_clusters_{N_CLS}_states_{N_STATES}_actions_{N_ACTIONS}_horizon_{H}_gamma_{GAMMA}.pickle', 'wb') as handle:\n",
        "    pickle.dump((calls, time_taken), handle)"
      ],
      "metadata": {
        "id": "ZRjMgcppgDi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "DJEzgQfKOHyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "FILE_PREFIX = '/usr/local/google/home/abheekghosh'  # for drive it may be different\n",
        "DATA = \"RANDOM\"\n",
        "\n",
        "if DATA == \"ARMMAN\":\n",
        "  N_STATES = 2\n",
        "  N_ACTIONS = 2\n",
        "  N_BEN = None  # if None, to be set based on the dataset, ~ 98,000\n",
        "  BUDGET = 1000  # number of interventions\n",
        "  N_CLS = 40  # number of clusters\n",
        "  GAMMA = 0.95\n",
        "elif DATA == \"ADVERSE\":\n",
        "  N_STATES = 5\n",
        "  N_ACTIONS = 2\n",
        "  N_BEN = 100\n",
        "  BUDGET = 50\n",
        "  N_CLS = 1\n",
        "  GAMMA = 0.999\n",
        "elif DATA == \"RANDOM\":\n",
        "  N_STATES = 5\n",
        "  N_ACTIONS = 2\n",
        "  N_BEN = 100000  # number of beneficiaries\n",
        "  BUDGET = 1000  # number of interventions\n",
        "  N_CLS = 40  # number of clusters\n",
        "  GAMMA = 0.95\n",
        "else:\n",
        "  assert(False)\n",
        "\n",
        "H_RANGE = np.concatenate([np.arange(2, 20), np.arange(20, 40, 4), np.arange(40, 80, 8), np.arange(80, 101, 10)])\n",
        "# H_RANGE = np.arange(10,101,10)\n",
        "# H_RANGE = np.array([5, 10])\n",
        "N_RUNS = 40  # number of runs for the experiement (48 clusters in the machine)\n",
        "\n",
        "# Algorithms\n",
        "ALGOS = ['rd', 'wi-if', 'wi-f', 'mf']\n",
        "ALGOS_NAMES = {'rd': 'Random', 'wi-if': 'WhittleIF', 'wi-f': 'WhittleF', 'mf': 'MeanField'}\n",
        "ALGORITHMS = {'rd': random, 'wi-if': whittle_infinite, 'wi-f': whittle_finite, 'mf': mean_field}"
      ],
      "metadata": {
        "id": "_E8mnlUHb-ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eFOOTHfrrVh"
      },
      "source": [
        "## runs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### parallel worker"
      ],
      "metadata": {
        "id": "g0881e9k6GFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def worker(P, R, C, Ni, state, rewards_mp, calls_mp, time_taken_mp):\n",
        "  # rewards, calls, and time taken for this run\n",
        "  reward = dict(); call = np.zeros(H); time_tk = dict()\n",
        "  for alg in ALGOS: \n",
        "    reward[alg] = 0\n",
        "    time_tk[alg] = np.zeros(H)\n",
        "\n",
        "  for t in range(H):\n",
        "    # print(t)\n",
        "    action = dict()\n",
        "    call[H-t-1] += 1\n",
        "    for alg in ALGOS:\n",
        "      start = time.process_time()\n",
        "      action[alg] = ALGORITHMS[alg](P, R, C, H-t+1, state[alg], Ni, GAMMA)\n",
        "      time_tk[alg][H-t-1] += (time.process_time() - start)\n",
        "      assert((action[alg]*C).sum() <= BUDGET + 1e-3)\n",
        "\n",
        "      reward[alg] += (GAMMA**(t-1))*compute_reward(alg, R, state[alg], action[alg])\n",
        "      state[alg] = next_state(P, state[alg], action[alg])\n",
        "      assert(state[alg].sum() == N_BEN)\n",
        "\n",
        "  rewards_mp.append(reward); calls_mp.append(call); time_taken_mp.append(time_tk)\n",
        "  print(f'{i_run}', end=' ')"
      ],
      "metadata": {
        "id": "Tq-UGLnFaj42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run the loop"
      ],
      "metadata": {
        "id": "rJGbK3Iv6LL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxEEGrUyoYfj"
      },
      "outputs": [],
      "source": [
        "# Select one of the following data sources\n",
        "if DATA == \"ARMMAN\":\n",
        "  P, R, C, Ni = get_ARMMAN_RMAB()\n",
        "  P = np.abs(P)  # due to rounding error some values were -0 or approx -1e-14, causing issues later\n",
        "elif DATA == \"ADVERSE\":\n",
        "  P, R, C, Ni = get_adverse_RMAB()\n",
        "\n",
        "\n",
        "# Rewards\n",
        "rewards = dict()\n",
        "for alg in ALGOS: rewards[alg] = np.zeros((H_RANGE.size, N_RUNS))\n",
        "\n",
        "# Time Taken\n",
        "time_taken = dict()\n",
        "calls = np.zeros(H_RANGE[-1])\n",
        "for alg in ALGOS: time_taken[alg] = np.zeros(H_RANGE[-1])\n",
        "\n",
        "for i_H in range(H_RANGE.size):\n",
        "  H = H_RANGE[i_H]  # horizon\n",
        "\n",
        "  # Multiprocessing\n",
        "  manager = multiprocessing.Manager()\n",
        "  rewards_mp = manager.list(); calls_mp = manager.list(); time_taken_mp = manager.list()\n",
        "  jobs = []  # multiprocessing jobs\n",
        "\n",
        "  for i_run in range(N_RUNS):\n",
        "    if DATA == \"RANDOM\":\n",
        "      P, R, C, Ni = get_random_RMAB()\n",
        "      P = np.abs(P)\n",
        "\n",
        "    if i_run % 10 == 0:\n",
        "      print(f\"Running H = {H} and run = {i_run}\")\n",
        "\n",
        "    state = dict()\n",
        "    if DATA == \"ADVERSE\":\n",
        "      random_start_state = np.zeros((N_CLS, N_STATES))\n",
        "      random_start_state[:,0] = N_BEN/2\n",
        "      random_start_state[:,2] = N_BEN/2\n",
        "    else:\n",
        "      random_start_state = get_random_start_state(Ni)\n",
        "    for alg in ALGOS: state[alg] = random_start_state  # start at same (random) state\n",
        "\n",
        "    proc = multiprocessing.Process(target=worker, args=(P, R, C, Ni, state, rewards_mp, calls_mp, time_taken_mp))\n",
        "    jobs.append(proc)\n",
        "    proc.start()\n",
        "\n",
        "  for proc in jobs:\n",
        "    proc.join()\n",
        "  \n",
        "  # combine the collected statistics from the parallel runs\n",
        "  for i_run in range(N_RUNS):\n",
        "    calls[:calls_mp[i_run].size] += calls_mp[i_run]\n",
        "    for alg in ALGOS: \n",
        "      rewards[alg][i_H][i_run] = rewards_mp[i_run][alg]\n",
        "      time_taken[alg][:time_taken_mp[i_run][alg].size] += time_taken_mp[i_run][alg]\n",
        "  for alg in ALGOS: rewards[alg][i_H] /= (H)\n",
        "\n",
        "  # save_backup()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards"
      ],
      "metadata": {
        "id": "3yq2V_MpFwno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot"
      ],
      "metadata": {
        "id": "l_UmStlNmyPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alg in ALGOS:\n",
        "  plt.plot(H_RANGE, rewards[alg].mean(axis=-1), label=ALGOS_NAMES[alg])\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"Length of Time Horizon\")\n",
        "plt.ylabel(\"Number of Engaged Beneficiaries / Reward\")\n",
        "plt.title(\"Average Reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LnqEk-o_m0Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for alg in ALGOS:\n",
        "  if alg == 'rd':\n",
        "    continue\n",
        "  plt.plot(H_RANGE, (rewards[alg] - rewards['rd']).mean(axis=-1), label=ALGOS_NAMES[alg])\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"Length of Time Horizon\")\n",
        "plt.ylabel(\"Reward minus Random's Reward\")\n",
        "plt.title(\"Reward of Algorithm minus Random's Reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Osjz6NAnnR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rewardsDiff = rewards['mf'] - rewards['wi-f']\n",
        "# plt.plot(H_RANGE, rewardsDiff.mean(axis=-1), 'b')\n",
        "# plt.fill_between(H_RANGE, \n",
        "#                  rewardsDiff.mean(axis=-1) + rewardsDiff.std(axis=-1), \n",
        "#                  rewardsDiff.mean(axis=-1) - rewardsDiff.std(axis=-1),\n",
        "#                  color='c')\n",
        "# plt.xlabel(\"Time horizon H\")\n",
        "# plt.ylabel(\"Difference b/w reward of MF and WI\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "lbYnKn5dnykn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for alg in ALGOS:\n",
        "  plt.plot(range(calls.size), time_taken[alg]/calls, label=ALGOS_NAMES[alg])\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"Length of Time Horizon\")\n",
        "plt.ylabel(\"Average Time Taken per Call (sec)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y3_WUFKZn8Bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KsBDDrNqZj4F",
        "wpRBc5iOMfJT",
        "Li5DRTLChTcY",
        "_b5lyLxPhe40",
        "wRKlu7GA4zTX",
        "P1Q1tOEeAoab",
        "guQ6iQFP29n6",
        "KltJjJdNmRfR",
        "BwG5TMYzmaJn",
        "2KFnMhbtdxHc",
        "_rsiHCptbnJO",
        "SJ4sogGUeT2c",
        "cR9RD5crbygu",
        "J1mTja1Lh9Tn",
        "fXOWZX5Haqtd",
        "rWdtWmPuroQW",
        "OoRzZUXkgEdh",
        "DJEzgQfKOHyv",
        "4eFOOTHfrrVh",
        "l_UmStlNmyPD"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}